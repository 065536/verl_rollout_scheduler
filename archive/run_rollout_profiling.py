#!/usr/bin/env python3
"""
é€šç”¨çš„Rollout Profilingå¯åŠ¨è„šæœ¬
æ”¯æŒä¸‰ç§è°ƒåº¦æ–¹å¼ï¼š
1. task_scheduler - å®æ—¶åŠ¨æ€è°ƒåº¦ï¼ˆæœ€å¤§ä¼˜å…ˆç­–ç•¥ï¼‰
2. bin_packing - é™æ€é¢„åˆ†é…è°ƒåº¦ï¼ˆBin Packingç®—æ³•ï¼‰
3. verl_default - VERLæ¡†æ¶é»˜è®¤è°ƒåº¦ï¼ˆæ•°æ®å¹¶è¡Œdispatchï¼‰
"""

import argparse
import csv
import json
import os
import sys
import time
from pathlib import Path
from statistics import mean, pstdev
from typing import List, Dict, Any
import numpy as np
import pandas as pd
import ray

# ç¡®ä¿åœ¨å¯¼å…¥å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ.setdefault("VLLM_WORKER_MULTIPROC_METHOD", "spawn")
os.environ.setdefault("TRUST_REMOTE_CODE", "1")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "verl"))

from transformers import AutoTokenizer

from rollout_profiling.workers.verl_worker import VERLRolloutWorker
from rollout_profiling.workers.vllm_worker import VLLMRolloutWorker
from rollout_profiling.utils.ema_predictor import EMALengthPredictor
from rollout_profiling.utils.scheduler import TaskScheduler
from rollout_profiling.utils.bin_packing import BinPackingScheduler
from rollout_profiling.utils.verl_utils import (
    prompts_to_dataproto,
    dataproto_to_responses,
    create_verl_rollout_config,
    setup_verl_environment,
)
from verl.protocol import pad_dataproto_to_divisor, unpad_dataproto
from verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup
from verl.workers.fsdp_workers import ActorRolloutRefWorker


def compute_basic_stats(values: List[float]) -> Dict[str, float]:
    if not values:
        return {}
    if len(values) == 1:
        return {
            "mean": float(values[0]),
            "std": 0.0,
            "min": float(values[0]),
            "max": float(values[0]),
        }
    return {
        "mean": float(mean(values)),
        "std": float(pstdev(values)),
        "min": float(min(values)),
        "max": float(max(values)),
    }


def normalize_prompt(prompt: Any) -> List[Dict[str, Any]] | None:
    """å°†å„ç§æ ¼å¼çš„promptè½¬æ¢ä¸ºæ¶ˆæ¯åˆ—è¡¨"""
    import pandas as pd  # local import to avoid issues when pandas absent

    while True:
        if isinstance(prompt, list):
            return prompt
        if isinstance(prompt, dict):
            return [prompt]
        if isinstance(prompt, str):
            return [{"role": "user", "content": prompt}]
        if isinstance(prompt, np.ndarray):
            if prompt.size == 0:
                return None
            if prompt.size == 1:
                prompt = prompt.item()
                continue
            prompt = prompt.tolist()
            continue
        if isinstance(prompt, pd.Series):
            if len(prompt) == 0:
                return None
            if len(prompt) == 1:
                prompt = prompt.iloc[0]
                continue
            prompt = prompt.tolist()
            continue
        break
    return None


def load_prompts_from_parquet(parquet_path: str, prompt_key: str = "prompt") -> List[Any]:
    """ä»parquetæ–‡ä»¶åŠ è½½prompts"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    print(f"âœ“ æ•°æ®é›†å¤§å°: {len(df)} æ¡")
    
    # æå–prompts
    if prompt_key in df.columns:
        prompts = df[prompt_key].tolist()
    else:
        # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„promptåˆ—
        possible_keys = ["messages", "input", "query", "question"]
        for key in possible_keys:
            if key in df.columns:
                print(f"âš ï¸  æœªæ‰¾åˆ°'{prompt_key}'åˆ—ï¼Œä½¿ç”¨'{key}'åˆ—")
                prompts = df[key].tolist()
                break
        else:
            raise ValueError(f"æœªæ‰¾åˆ°promptåˆ—ï¼Œå¯ç”¨åˆ—: {df.columns.tolist()}")
    
    # å¤„ç†promptsæ ¼å¼
    processed_prompts = []
    for i, prompt in enumerate(prompts):
        normalized = normalize_prompt(prompt)
        if normalized is None:
            print(f"âš ï¸  Prompt {i} æ ¼å¼æœªçŸ¥: {type(prompt)}, è·³è¿‡")
            continue
        processed_prompts.append(normalized)
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(processed_prompts)} ä¸ªprompts")
    return processed_prompts


def extract_worker_rows(scheduler_name: str, results: Dict) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    if scheduler_name == "task_scheduler":
        worker_stats = results.get('worker_stats', {})
        for worker_id, stats in worker_stats.items():
            rows.append({
                'worker_id': worker_id,
                'num_responses': stats.get('num_responses', 0),
                'total_tokens': stats.get('total_tokens', 0),
                'total_time': stats.get('total_time', 0.0),
                'pure_generation_time': stats.get('pure_generation_time', 0.0),
            })
    else:
        worker_results = results.get('worker_results', [])
        for entry in worker_results:
            responses = entry.get('responses', [])
            total_tokens = sum(resp.get('response_length', 0) for resp in responses)
            rows.append({
                'worker_id': entry.get('worker_id'),
                'num_responses': entry.get('num_responses', len(responses)),
                'total_tokens': total_tokens,
                'total_time': entry.get('worker_duration'),
                'pure_generation_time': entry.get('pure_generation_duration'),
            })
    return rows


def run_task_scheduler_mode(
    prompts: List[Any],
    num_workers: int,
    worker_type: str,
    model_path: str,
    max_tokens: int,
    remaining_rounds: int = 1,
    **kwargs
):
    """ä½¿ç”¨TaskSchedulerè¿›è¡Œå®æ—¶åŠ¨æ€è°ƒåº¦"""
    print(f"\n{'='*70}")
    print(f"ğŸš€ å¯åŠ¨ TaskScheduler æ¨¡å¼ï¼ˆå®æ—¶åŠ¨æ€è°ƒåº¦ï¼‰")
    print(f"{'='*70}")
    
    # åˆå§‹åŒ–EMAé¢„æµ‹å™¨
    num_prompts = len(prompts)
    ema_predictor = EMALengthPredictor(
        num_prompts=num_prompts,
        initial_length=512.0,
        alpha=0.3
    )
    
    # åˆ›å»ºTaskScheduler
    ema_state = ema_predictor.export_state()
    scheduler = TaskScheduler.remote(
        ema_state=ema_state,
        remaining_rounds=remaining_rounds,
    )
    
    # åˆ›å»ºworkers
    print(f"\nåˆ›å»º {num_workers} ä¸ª {worker_type} workers...")
    workers = []
    for i in range(num_workers):
        if worker_type == "verl":
            worker = VERLRolloutWorker.remote(
                worker_id=i,
                model_path=model_path,
                max_tokens=max_tokens,
                **kwargs
            )
        else:
            worker = VLLMRolloutWorker.remote(
                worker_id=i,
                model_path=model_path,
                max_tokens=max_tokens,
                **kwargs
            )
        workers.append(worker)
    
    # å¯åŠ¨workerå¤„ç†ä»»åŠ¡
    print(f"\nå¼€å§‹å¤„ç†ä»»åŠ¡...")
    start_time = time.time()
    
    futures = []
    for worker in workers:
        future = worker.process_task_queue.remote(
            scheduler=scheduler,
            prompts=prompts,
            temperature=kwargs.get("temperature", 1.0),
            top_k=kwargs.get("top_k", -1),
            top_p=kwargs.get("top_p", 1.0),
        )
        futures.append(future)
    
    # ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
    results = ray.get(futures)
    end_time = time.time()
    
    # è·å–æœ€ç»ˆç»“æœ
    final_results = ray.get(scheduler.get_all_results.remote())
    
    print(f"\n{'='*70}")
    print(f"âœ… TaskScheduler æ¨¡å¼å®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"å¤„ç†çš„ä»»åŠ¡æ•°: {sum(r['processed_tasks'] for r in results)}")
    print(f"æ€»å“åº”æ•°: {len(final_results['responses'])}")
    
    return final_results


def run_bin_packing_mode(
    prompts: List[Any],
    num_workers: int,
    worker_type: str,
    model_path: str,
    max_tokens: int,
    schedule_csv: str = None,
    **kwargs
):
    """ä½¿ç”¨BinPackingSchedulerè¿›è¡Œé™æ€é¢„åˆ†é…è°ƒåº¦"""
    print(f"\n{'='*70}")
    print(f"ğŸš€ å¯åŠ¨ BinPacking æ¨¡å¼ï¼ˆé™æ€é¢„åˆ†é…è°ƒåº¦ï¼‰")
    print(f"{'='*70}")
    
    # åˆå§‹åŒ–EMAé¢„æµ‹å™¨ï¼ˆç”¨äºé¢„æµ‹é•¿åº¦ï¼‰
    num_prompts = len(prompts)
    ema_predictor = EMALengthPredictor(
        num_prompts=num_prompts,
        initial_length=512.0,
        alpha=0.3
    )
    
    # è·å–é¢„æµ‹é•¿åº¦
    prompt_lengths = ema_predictor.get_all_expected_lengths()
    
    # å¦‚æœæä¾›äº†CSVæ–‡ä»¶ï¼Œä»CSVåŠ è½½è°ƒåº¦è®¡åˆ’
    if schedule_csv and os.path.exists(schedule_csv):
        print(f"ğŸ“‹ ä»CSVæ–‡ä»¶åŠ è½½è°ƒåº¦è®¡åˆ’: {schedule_csv}")
        df = pd.read_csv(schedule_csv)
        worker_assignments = []
        for i in range(num_workers):
            worker_row = df[df['worker_id'] == i]
            if not worker_row.empty:
                import ast
                prompt_ids = ast.literal_eval(worker_row.iloc[0]['prompt_ids'])
                worker_assignments.append(prompt_ids)
            else:
                worker_assignments.append([])
        print(f"âœ“ ä»CSVåŠ è½½äº† {num_workers} ä¸ªworkerçš„åˆ†é…è®¡åˆ’")
    else:
        # ä½¿ç”¨BinPackingSchedulerç”Ÿæˆè°ƒåº¦è®¡åˆ’
        print(f"ğŸ“‹ ä½¿ç”¨BinPackingSchedulerç”Ÿæˆè°ƒåº¦è®¡åˆ’...")
        bin_packer = BinPackingScheduler(num_workers=num_workers)
        worker_assignments = bin_packer.schedule_prompts(prompt_lengths)
    
    # åˆ›å»ºworkers
    print(f"\nåˆ›å»º {num_workers} ä¸ª {worker_type} workers...")
    workers = []
    for i in range(num_workers):
        if worker_type == "verl":
            worker = VERLRolloutWorker.remote(
                worker_id=i,
                model_path=model_path,
                max_tokens=max_tokens,
                **kwargs
            )
        else:
            worker = VLLMRolloutWorker.remote(
                worker_id=i,
                model_path=model_path,
                max_tokens=max_tokens,
                **kwargs
            )
        workers.append(worker)
    
    # ä¸ºæ¯ä¸ªworkeråˆ†é…promptså¹¶ç”Ÿæˆ
    print(f"\nå¼€å§‹å¤„ç†ä»»åŠ¡...")
    start_time = time.time()
    
    futures = []
    for i, worker in enumerate(workers):
        assigned_prompt_ids = worker_assignments[i]
        if assigned_prompt_ids:
            assigned_prompts = [prompts[pid] for pid in assigned_prompt_ids]
            future = worker.generate.remote(
                prompts=assigned_prompts,
                n_samples=1,
                temperature=kwargs.get("temperature", 1.0),
                top_k=kwargs.get("top_k", -1),
                top_p=kwargs.get("top_p", 1.0),
            )
            futures.append(future)
        else:
            print(f"âš ï¸  Worker {i} æ²¡æœ‰åˆ†é…ä»»ä½•prompts")
    
    # ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
    results = ray.get(futures)
    end_time = time.time()
    
    # æ±‡æ€»ç»“æœ
    all_responses = []
    for result in results:
        if result and 'responses' in result:
            all_responses.extend(result['responses'])
    
    print(f"\n{'='*70}")
    print(f"âœ… BinPacking æ¨¡å¼å®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"æ€»å“åº”æ•°: {len(all_responses)}")
    
    return {
        'responses': all_responses,
        'worker_results': results,
    }


def _build_process_layout(total_workers: int, nnodes: int, gpus_per_node: int) -> List[int]:
    """è®¡ç®—RayResourcePoolçš„ process_on_nodes å¸ƒå±€"""
    remaining = total_workers
    layout: List[int] = []
    for _ in range(max(nnodes, 1)):
        if remaining <= 0:
            break
        assign = min(gpus_per_node, remaining) if gpus_per_node > 0 else remaining
        layout.append(assign)
        remaining -= assign
    while remaining > 0:
        assign = min(gpus_per_node, remaining) if gpus_per_node > 0 else remaining
        layout.append(assign)
        remaining -= assign
    return [c for c in layout if c > 0]


def run_verl_default_mode(
    prompts: List[Any],
    num_workers: int,
    model_path: str,
    max_tokens: int,
    nnodes: int = 1,
    gpus_per_node: int = 1,
    **kwargs,
):
    """ä½¿ç”¨VERLå®˜æ–¹Ray WorkerGroupè°ƒåº¦"""
    print(f"\n{'='*70}")
    print(f"ğŸš€ å¯åŠ¨ VERLé»˜è®¤è°ƒåº¦ æ¨¡å¼ï¼ˆRay WorkerGroup + generate_sequencesï¼‰")
    print(f"{'='*70}")

    setup_verl_environment()
    total_world_size = num_workers
    layout = _build_process_layout(total_world_size, nnodes, gpus_per_node)
    if sum(layout) != total_world_size:
        raise ValueError(
            f"èµ„æºå¸ƒå±€å¼‚å¸¸: æœŸæœ›ä¸–ç•Œå°ºå¯¸ {total_world_size}, å®é™… {sum(layout)} (layout={layout})"
        )
    print(f"Rayèµ„æºå¸ƒå±€ï¼ˆprocess_on_nodesï¼‰: {layout}")

    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    config = create_verl_rollout_config(
        model_path=model_path,
        max_tokens=max_tokens,
        temperature=kwargs.get("temperature", 1.0),
        top_k=kwargs.get("top_k", -1),
        top_p=kwargs.get("top_p", 1.0),
        gpu_memory=kwargs.get("gpu_memory", 0.5),
    )
    config.actor.fsdp_config["fsdp_size"] = total_world_size
    config.rollout["data_parallel_size"] = total_world_size

    ray_cls = RayClassWithInitArgs(
        cls=ray.remote(ActorRolloutRefWorker),
        config=config,
        role="rollout",
    )
    resource_pool = RayResourcePool(process_on_nodes=layout, use_gpu=True, max_colocate_count=1)
    worker_group = RayWorkerGroup(
        resource_pool=resource_pool,
        ray_cls_with_init=ray_cls,
        device_name="cuda",
    )
    print("â³ åˆå§‹åŒ–VERL WorkerGroupæ¨¡å‹...")
    worker_group.init_model()

    dataproto = prompts_to_dataproto(prompts, tokenizer)
    attention_mask = dataproto.batch["attention_mask"]
    original_prompt_lengths = [int(mask.sum().item()) for mask in attention_mask]

    padded_proto, pad_size = pad_dataproto_to_divisor(dataproto, worker_group.world_size)
    print(f"å‘é€ DataProtoï¼ˆ{len(dataproto)} æ¡ï¼Œpadding={pad_size}ï¼‰åˆ° WorkerGroup")

    start_time = time.time()
    output_padded = worker_group.generate_sequences(padded_proto)
    output = unpad_dataproto(output_padded, pad_size=pad_size)
    total_duration = time.time() - start_time

    timing_info = output.meta_info.get("timing", {})
    responses = dataproto_to_responses(
        dataproto=output,
        tokenizer=tokenizer,
        prompt_ids=list(range(len(prompts))),
        original_prompt_lengths=original_prompt_lengths,
    )
    total_tokens = sum(resp.get("response_length", 0) for resp in responses)

    print(f"\n{'='*70}")
    print("âœ… VERLé»˜è®¤è°ƒåº¦ æ¨¡å¼å®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»è€—æ—¶: {total_duration:.2f} ç§’ | å“åº”æ¡æ•°: {len(responses)}")
    if timing_info:
        print(f"VERLè¿”å›çš„ç”Ÿæˆè€—æ—¶ç»Ÿè®¡: {timing_info}")

    worker_results = [
        {
            "worker_id": "verl_dp_group",
            "num_responses": len(responses),
            "total_tokens": total_tokens,
            "worker_duration": total_duration,
            "pure_generation_duration": timing_info.get("generation_timing/mean", total_duration),
            "timing_info": timing_info,
        }
    ]

    return {
        "responses": responses,
        "worker_results": worker_results,
        "timing": timing_info,
    }


def main():
    parser = argparse.ArgumentParser(description="Rollout Profilingå¯åŠ¨è„šæœ¬")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--scheduler", type=str, required=True,
                        choices=["task_scheduler", "bin_packing", "verl_default"],
                        help="è°ƒåº¦æ–¹å¼: task_scheduler, bin_packing, verl_default")
    parser.add_argument("--worker_type", type=str, default="vllm",
                        choices=["verl", "vllm"],
                        help="Workerç±»å‹: verl æˆ– vllm")
    parser.add_argument("--model_path", type=str, required=True,
                        help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--num_workers", type=int, default=8,
                        help="Workeræ•°é‡")
    parser.add_argument("--max_tokens", type=int, default=8192,
                        help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--nnodes", type=int, default=int(os.environ.get("NNODES", "1")),
                        help="Rayé›†ç¾¤èŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤è¯»å–NNODESç¯å¢ƒå˜é‡ï¼‰")
    parser.add_argument("--gpus_per_node", type=int, default=int(os.environ.get("NGPUS_PER_NODE", "1")),
                        help="æ¯ä¸ªèŠ‚ç‚¹å¯ç”¨GPUæ•°é‡ï¼ˆé»˜è®¤è¯»å–NGPUS_PER_NODEç¯å¢ƒå˜é‡ï¼‰")
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--dataset", type=str,
                        default="data/dapo_math_subset_128.parquet",
                        help="æ•°æ®é›†parquetæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--prompt_key", type=str, default="prompt",
                        help="promptåˆ—å")
    
    # BinPackingç‰¹å®šå‚æ•°
    parser.add_argument("--schedule_csv", type=str, default=None,
                        help="BinPackingè°ƒåº¦è®¡åˆ’CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
    
    # TaskSchedulerç‰¹å®šå‚æ•°
    parser.add_argument("--remaining_rounds", type=int, default=1,
                        help="æ¯ä¸ªpromptç”Ÿæˆçš„è½®æ•°ï¼ˆTaskScheduleræ¨¡å¼ï¼‰")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--temperature", type=float, default=1.0,
                        help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top_k", type=int, default=-1,
                        help="Top-ké‡‡æ ·")
    parser.add_argument("--top_p", type=float, default=1.0,
                        help="Nucleusé‡‡æ ·")
    parser.add_argument("--gpu_memory", type=float, default=0.5,
                        help="GPUå†…å­˜ä½¿ç”¨ç‡")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="profiling_results",
                        help="ç»“æœè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–Ray
    if not ray.is_initialized():
        init_kwargs = {"ignore_reinit_error": True}
        ray_address = os.environ.get("RAY_ADDRESS")
        if ray_address:
            init_kwargs["address"] = ray_address
            print(f"è¿æ¥ç°æœ‰Rayé›†ç¾¤: {ray_address}")
        else:
            init_kwargs["num_cpus"] = max(args.num_workers + 2, 2)
        ray.init(**init_kwargs)
        print(f"âœ“ Rayåˆå§‹åŒ–å®Œæˆ")
    
    # åŠ è½½æ•°æ®é›†
    prompts = load_prompts_from_parquet(args.dataset, args.prompt_key)
    
    # å‡†å¤‡kwargs
    kwargs = {
        "temperature": args.temperature,
        "top_k": args.top_k,
        "top_p": args.top_p,
        "gpu_memory": args.gpu_memory,
    }
    
    overall_start_time = time.time()

    # æ ¹æ®è°ƒåº¦æ–¹å¼è¿è¡Œ
    if args.scheduler == "task_scheduler":
        results = run_task_scheduler_mode(
            prompts=prompts,
            num_workers=args.num_workers,
            worker_type=args.worker_type,
            model_path=args.model_path,
            max_tokens=args.max_tokens,
            remaining_rounds=args.remaining_rounds,
            **kwargs
        )
    elif args.scheduler == "bin_packing":
        results = run_bin_packing_mode(
            prompts=prompts,
            num_workers=args.num_workers,
            worker_type=args.worker_type,
            model_path=args.model_path,
            max_tokens=args.max_tokens,
            schedule_csv=args.schedule_csv,
            **kwargs
        )
    elif args.scheduler == "verl_default":
        results = run_verl_default_mode(
            prompts=prompts,
            num_workers=args.num_workers,
            model_path=args.model_path,
            max_tokens=args.max_tokens,
            nnodes=args.nnodes,
            gpus_per_node=args.gpus_per_node,
            **kwargs
        )
    else:
        raise ValueError(f"æœªçŸ¥çš„è°ƒåº¦æ–¹å¼: {args.scheduler}")
    
    overall_duration = time.time() - overall_start_time

    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    base_name = f"profiling_{args.scheduler}_{args.worker_type}_{timestamp}"
    responses_csv = os.path.join(args.output_dir, f"{base_name}_responses.csv")
    workers_csv = os.path.join(args.output_dir, f"{base_name}_workers.csv")
    summary_json = os.path.join(args.output_dir, f"{base_name}_summary.json")
    
    responses = results.get('responses', [])
    responses_df = pd.DataFrame(responses) if responses else pd.DataFrame()
    if not responses_df.empty:
        responses_df.to_csv(
            responses_csv,
            index=False,
            quoting=csv.QUOTE_MINIMAL,
            escapechar="\\",
        )
    else:
        responses_csv = None
    
    worker_rows = extract_worker_rows(args.scheduler, results)
    worker_df = pd.DataFrame(worker_rows) if worker_rows else pd.DataFrame()
    if not worker_df.empty:
        worker_df.to_csv(
            workers_csv,
            index=False,
            quoting=csv.QUOTE_MINIMAL,
            escapechar="\\",
        )
    else:
        workers_csv = None

    response_length_stats = {}
    if not responses_df.empty and 'response_length' in responses_df.columns:
        lengths = responses_df['response_length'].dropna().astype(float).tolist()
        response_length_stats = compute_basic_stats(lengths)

    worker_timing_stats = {}
    if not worker_df.empty and 'total_time' in worker_df.columns:
        timings = worker_df['total_time'].dropna().astype(float).tolist()
        worker_timing_stats = compute_basic_stats(timings)
        worker_timing_stats["num_workers"] = len(timings)

    summary = {
        "scheduler": args.scheduler,
        "worker_type": args.worker_type,
        "model_path": args.model_path,
        "num_workers": args.num_workers,
        "num_prompts": len(prompts),
        "total_responses": len(responses),
        "total_time_sec": overall_duration,
        "response_length": response_length_stats,
        "worker_timing": worker_timing_stats,
        "files": {
            "responses_csv": responses_csv,
            "workers_csv": workers_csv,
        },
    }
    with open(summary_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("\nâœ“ è¾“å‡ºæ–‡ä»¶:")
    if responses_csv:
        print(f"  - Responses: {responses_csv}")
    if workers_csv:
        print(f"  - Workers:   {workers_csv}")
    print(f"  - Summary:   {summary_json}")
    print(f"\nğŸ‰ å®Œæˆï¼æ€»è€—æ—¶ {overall_duration:.2f} ç§’")


if __name__ == "__main__":
    main()

